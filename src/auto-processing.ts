import { createLLMProvider } from './llm-provider.js';
import { getSources, saveMoment } from './storage.js';
import { semanticSearch as rawSemanticSearch } from './search.js';
import type { SourceRecord, MomentRecord, QualityType, ShotType } from './types.js';
import { createBatchFramePrompt, createBatchRevisionPrompt } from './prompts.js';

export interface FrameFragment {
  sourceId: string;
  start?: number;
  end?: number;
  text?: string;
}

export interface DraftFrame {
  sources: FrameFragment[];
  shot: ShotType;
  qualities: Array<{ type: QualityType; manifestation: string }>;
}

export interface AssignFrame {
  emoji: string;
  summary: string;
  shot: ShotType;
  qualities: Array<{ type: QualityType; manifestation: string }>;
  sources: FrameFragment[];
}

export interface CritiqueResult {
  pass: boolean;
  reasons: string[];
}

export interface AutoProcessingResult {
  success: boolean;
  created?: MomentRecord;
  error?: string;
  warning?: string;
}

const debugLogs: string[] = [];
function addDebugLog(msg: string) { debugLogs.push(msg); }
export function getAutoProcessingDebugLogs() { return debugLogs; }

export class AutoProcessor {
  private lastBatch: SourceRecord[] = [];
  protected llmProvider = createLLMProvider();

  // Main entry: auto-frame with context enrichment and validation
  async autoFrameSources(options: {
    sourceIds?: string[];
    timeWindowMinutes?: number;
    linkageSourceId?: string;
    maxBatchSize?: number;
    semanticSimilarity?: boolean;
  } = {}): Promise<AutoProcessingResult[]> {
    const allSources = await getSources();
    let batch: SourceRecord[];
    if (options.sourceIds && options.sourceIds.length > 0) {
      const idSet = new Set(options.sourceIds);
      batch = allSources.filter(s => idSet.has(s.id));
    } else {
      batch = this.selectBatch(allSources, options);
    }
    this.lastBatch = batch;
    if (batch.length === 0) {
      return [{ success: false, error: 'No unframed sources found for auto-framing.' }];
    }

    let tries = 0;
    const maxTries = 3;
    let frames: AssignFrame[] = [];
    let validation: { errors: string[]; warnings: string[] } = { errors: [], warnings: [] };
    let lastCritique: string[] = [];
    let lastPrompt = '';
    const fullSourceText = batch.map(s => s.content).join('\n\n');
    while (tries < maxTries) {
      if (tries === 0) {
        // 1. Frame the sources (initial batch prompt)
        lastPrompt = await createBatchFramePrompt(batch);
        const batchResponse = await this.llmProvider.complete(lastPrompt, { maxTokens: 3000 });
        frames = this.parseBatchFrameResponse(batchResponse);
      } else {
        // Batch revision prompt for all frames at once
        lastPrompt = createBatchRevisionPrompt(frames, lastCritique, batch, fullSourceText);
        const revisionResponse = await this.llmProvider.complete(lastPrompt, { maxTokens: 3000 });
        frames = this.parseBatchFrameResponse(revisionResponse);
      }

      // 2. Programmatic validation (schema, coverage, allowed types)
      validation = await this.validateBatchFrames(frames, batch);
      if (validation.errors.length === 0) {
        break;
      } else {
        lastCritique = [...(validation.errors || []), ...(validation.warnings || [])];
        tries++;
      }
    }

    if (validation.errors.length === 0) {
      // 4. Save moments
      const results: AutoProcessingResult[] = [];
      for (const frame of frames) {
        const moment = await saveMoment({
          id: `mom_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
          emoji: frame.emoji,
          summary: frame.summary,
          qualities: frame.qualities,
          shot: frame.shot,
          sources: frame.sources.map((frag: FrameFragment) => {
            const src = batch.find(s => s.id === frag.sourceId);
            return {
              sourceId: frag.sourceId,
              start: frag.start,
              end: frag.end,
              text: frag.text || (src && typeof frag.start === 'number' && typeof frag.end === 'number' ? src.content.slice(frag.start, frag.end) : '')
            };
          }),
          created: new Date().toISOString(),
          experiencer: batch[0]?.experiencer || 'unknown',
          reviewed: false,
          autoGenerated: true,
        });
        results.push({ success: true, created: moment, warning: (validation.warnings.length > 0 ? validation.warnings.join('; ') : undefined) });
      }
      return results;
    } else {
      return [{ success: false, error: 'Batch did not pass validation after 3 attempts: ' + [...(validation.errors || []), ...(validation.warnings || [])].join('; ') }];
    }
  }

  // --- Batch selection logic ---
  selectBatch(sources: SourceRecord[], options: any): SourceRecord[] {
    const now = Date.now();
    let batch = sources.filter(() => !this.isFramed());
    // 1. Time window (created or when)
    if (options.timeWindowMinutes) {
      const cutoff = now - options.timeWindowMinutes * 60 * 1000;
      batch = batch.filter(s => {
        const t = new Date(s.created).getTime();
        return t >= cutoff;
      });
    }
    // 2. Linkage (reflects_on)
    if (options.linkageSourceId) {
      batch = batch.filter(s => s.reflects_on?.includes(options.linkageSourceId));
    }
    // 3. Semantic similarity (if too many)
    if (options.maxBatchSize && batch.length > options.maxBatchSize) {
      if (options.semanticSimilarity) {
        // Use semantic search to cluster
        // For now, just pick the top-N by semantic similarity to the first source
        // (Replace with real clustering if available)
        return batch; // We'll handle this in a future improvement
      } else {
        batch = batch.slice(0, options.maxBatchSize);
      }
    }
    return batch;
  }

  isFramed(): boolean {
    // TODO: Implement logic to check if a source is already framed (referenced by a moment)
    // For now, assume all sources are unframed
    return false;
  }

  // Prompt construction for auto-framing and assignment
  // See: FRAMED_MOMENTS.md and 5 - Experiments/*.md for theory, criteria, and examples
  // Validation Framework (13 criteria):
  // 1. Voice recognition ("that's how I talk")
  // 2. Experiential completeness
  // 3. Visual anchorability
  // 4. Temporal flow implied
  // 5. Emotional atmosphere preserved
  // 6. Self-containment
  // 7. Narrative coherence
  // 8. Causal logic
  // 9. Temporal knowledge accuracy
  // 10. No invented details
  // 11. Voice pattern fidelity
  // 12. Minimal transformation
  // 13. Physical/sensory grounding
  //
  // Prompt improvements: minimal transformation, authentic voice, physical/sensory grounding, iterative refinement, avoid over-interpretation, preserve original order, use examples if needed.

  // --- Context enrichment ---
  async findContextForFrame(fragments: FrameFragment[], batch: SourceRecord[]): Promise<string> {
    // Use the actual text of the fragments as the query, or the whole source if fragment is short
    let queryText = '';
    let queryType = 'fragment';
    if (fragments.length === 1) {
      const frag = fragments[0];
      const src = batch.find(s => s.id === frag.sourceId);
      if (src) {
        const fragText = src.content.slice(frag.start, frag.end);
        if (fragText.length < 40) {
          queryText = src.content;
          queryType = 'source';
        } else {
          queryText = fragText;
        }
      }
    }
    if (!queryText) {
      // Fallback: concatenate all fragment texts
      queryText = fragments.map(f => {
        const src = batch.find(s => s.id === f.sourceId);
        return src ? src.content.slice(f.start, f.end) : '';
      }).join(' ');
      queryType = 'multi-fragment';
    }
    // Debug: log the query text and type
    addDebugLog(`[semantic search] QueryType: ${queryType}, Query: ${queryText}`);
    // Use all sources for semantic search
    const { getSources } = await import('./storage.js');
    const allRecords = await getSources();
    addDebugLog(`[semantic search] Using all sources (${allRecords.length}) for context search.`);
    const { loadEmbeddings } = await import('./embeddings.js');
    const embeddings = await loadEmbeddings();
    const results = await rawSemanticSearch(queryText, allRecords, embeddings, 3, queryType);
    function hasContent(obj: unknown): obj is { content: string } {
      return typeof obj === 'object' && obj !== null && 'content' in obj && typeof (obj as any).content === 'string';
    }
    let contextStrings: string[] = [];
    if (Array.isArray(results)) {
      contextStrings = (results as unknown[]).filter(hasContent).map(r => r.content);
    } else if (results && 'groups' in results) {
      contextStrings = (results as { groups: { items: unknown[] }[] }).groups.flatMap(g => g.items.filter(hasContent).map(r => r.content));
    }
    // Add reflects_on sources if present
    const reflectsOnTexts: string[] = [];
    for (const frag of fragments) {
      const src = batch.find(s => s.id === frag.sourceId);
      if (src && Array.isArray(src.reflects_on) && src.reflects_on.length > 0) {
        for (const refId of src.reflects_on) {
          const ref = allRecords.find(r => r.id === refId);
          if (ref && ref.content) {
            reflectsOnTexts.push(`[reflects_on: ${ref.id}] ${ref.content}`);
          }
        }
      }
    }
    if (reflectsOnTexts.length > 0) {
      addDebugLog(`[reflects_on] Added context from reflects_on sources: ${reflectsOnTexts.map(t => t.slice(0, 80)).join(' | ')}`);
    }
    // Compose context block
    let contextBlock = '';
    if (reflectsOnTexts.length > 0) {
      contextBlock += 'REFLECTED SOURCES (referenced by this source):\n' + reflectsOnTexts.join('\n---\n') + '\n';
    }
    if (contextStrings.length > 0) {
      contextBlock += 'SEMANTIC CONTEXT (similar sources):\n' + contextStrings.join('\n---\n') + '\n';
    }
    if (!contextBlock) contextBlock = '[No additional context found]';
    addDebugLog(`[context block] For frame: ${contextBlock.slice(0, 200)}`);
    return contextBlock;
  }

  // --- Response parsing ---
  parseSplitResponse(response: string): DraftFrame[] {
    try {
      const jsonMatch = response.match(/\[.*\]/s);
      if (!jsonMatch) throw new Error('No JSON array found');
      const arr = JSON.parse(jsonMatch[0]);
      // New schema: array of { text }
      if (Array.isArray(arr) && arr.length > 0 && typeof arr[0] === 'object' && 'text' in arr[0]) {
        const batch = this.lastBatch || [];
        const source = batch[0];
        if (!source) return [];
        const usedRanges: Array<{ start: number; end: number; text: string }> = [];
        const usedTexts = new Set<string>();
        let lastIndex = 0;
        const draftFrames: DraftFrame[] = [];
        const errors: string[] = [];
        const warnings: string[] = [];
        for (const fragObj of arr) {
          const fragText = fragObj.text;
          if (!fragText || typeof fragText !== 'string' || fragText.trim().length === 0) {
            errors.push('Fragment is empty or whitespace.');
            continue;
          }
          if (usedTexts.has(fragText)) {
            errors.push(`Duplicate fragment: "${fragText}"`);
            continue;
          }
          const idx = source.content.indexOf(fragText, lastIndex);
          if (idx === -1) {
            errors.push(`Fragment not found in source: "${fragText}"`);
            continue;
          }
          const start = idx;
          const end = idx + fragText.length;
          usedRanges.push({ start, end, text: fragText });
          usedTexts.add(fragText);
          lastIndex = end; // move forward to avoid overlapping
          draftFrames.push({
            sources: [{ sourceId: source.id, start, end }],
            shot: '' as ShotType,
            qualities: []
          });
        }
        // Sort ranges by start
        usedRanges.sort((a, b) => a.start - b.start);
        // Check for overlaps
        for (let i = 1; i < usedRanges.length; i++) {
          if (usedRanges[i].start < usedRanges[i - 1].end) {
            errors.push(`Overlapping fragments: [${usedRanges[i - 1].start}, ${usedRanges[i - 1].end}) and [${usedRanges[i].start}, ${usedRanges[i].end})`);
          }
        }
        // Check for gaps
        let lastEnd = 0;
        for (const range of usedRanges) {
          if (range.start > lastEnd) {
            const gapText = source.content.slice(lastEnd, range.start);
            if (gapText.trim().length > 0) {
              warnings.push(`Unframed text at: [${lastEnd}, ${range.start}) -> "${gapText.trim()}"`);
            }
          }
          lastEnd = Math.max(lastEnd, range.end);
        }
        if (lastEnd < source.content.length) {
          const gapText = source.content.slice(lastEnd);
          if (gapText.trim().length > 0) {
            warnings.push(`Unframed text at: [${lastEnd}, ${source.content.length}) -> "${gapText.trim()}"`);
          }
        }
        // Attach errors/warnings to frames for downstream logic
        if (errors.length > 0 || warnings.length > 0) {
          (draftFrames as any)._validation = { errors, warnings };
        }
        return draftFrames;
      }
      // Fallback: old schema
      return arr;
    } catch (e) {
      return [];
    }
  }

  parseAssignResponse(response: string): AssignFrame {
    try {
      const jsonMatch = response.match(/\{[\s\S]*\}/);
      if (!jsonMatch) throw new Error('No JSON found');
      return JSON.parse(jsonMatch[0]);
    } catch (e) {
      return { emoji: '', summary: '', shot: '' as ShotType, qualities: [], sources: [] };
    }
  }

  parseCritiqueResponse(response: string): CritiqueResult {
    try {
      const jsonMatch = response.match(/\{[\s\S]*\}/);
      if (!jsonMatch) throw new Error('No JSON found');
      const parsed = JSON.parse(jsonMatch[0]);
      return { pass: !!parsed.pass, reasons: parsed.reasons || [] };
    } catch (e) {
      return { pass: false, reasons: ['Failed to parse critique response'] };
    }
  }

  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  async autoWeaveMoments(momentIds: string[]): Promise<{ success: boolean; created?: any; error?: string }> {
    // 1. Gather the batch of moments
    const { getMoments, saveScene } = await import('./storage.js');
    const allMoments = await getMoments();
    const batch = allMoments.filter(m => momentIds.includes(m.id));
    if (batch.length === 0) {
      return { success: false, error: 'No valid moments found for auto-weaving.' };
    }

    // 2. Build the LLM prompt
    const { createBatchWeavePrompt } = await import('./prompts.js');
    const prompt = createBatchWeavePrompt(batch);
    const llmResponse = await this.llmProvider.complete(prompt, { maxTokens: 2000 });

    // 3. Parse the LLM output (expecting an array of scenes)
    let scenes: any[] = [];
    try {
      const jsonMatch = llmResponse.match(/\[.*\]/s);
      if (!jsonMatch) throw new Error('No JSON array found');
      scenes = JSON.parse(jsonMatch[0]);
    } catch (e) {
      return { success: false, error: 'Failed to parse LLM scene output: ' + (e instanceof Error ? e.message : String(e)) };
    }

    // 4. Validate scenes
    const batchIds = new Set(batch.map(m => m.id));
    const allowedShots = [
      'moment-of-recognition',
      'sustained-attention',
      'crossing-threshold',
      'peripheral-awareness',
      'directed-momentum',
      'holding-opposites'
    ];
    const results: { success: boolean; created?: any; error?: string }[] = [];
    for (const scene of scenes) {
      // Validate required fields
      if (!scene || typeof scene !== 'object' ||
        typeof scene.emoji !== 'string' ||
        typeof scene.summary !== 'string' ||
        typeof scene.shot !== 'string' ||
        !Array.isArray(scene.momentIds) ||
        typeof scene.narrative !== 'string') {
        results.push({ success: false, error: 'Scene missing required fields.' });
        continue;
      }
      // Validate shot type
      if (!allowedShots.includes(scene.shot)) {
        results.push({ success: false, error: `Invalid shot type: ${scene.shot}` });
        continue;
      }
      // Validate all momentIds exist in batch
      if (!scene.momentIds.every((id: string) => batchIds.has(id))) {
        results.push({ success: false, error: 'Scene references momentIds not in batch.' });
        continue;
      }
      // Validate summary: 5-7 words, verb-forward (basic check: starts with a verb or action word)
      const wordCount = scene.summary.trim().split(/\s+/).length;
      if (wordCount < 5 || wordCount > 7) {
        results.push({ success: false, error: `Scene summary not 5-7 words: "${scene.summary}"` });
        continue;
      }
      // Optionally: check for verb-forward (could use a simple regex for now)
      // Save the scene
      const saved = await saveScene({
        id: `sce_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
        emoji: scene.emoji,
        summary: scene.summary,
        shot: scene.shot,
        momentIds: scene.momentIds,
        narrative: scene.narrative,
        created: new Date().toISOString(),
        experiencer: batch[0]?.experiencer || 'unknown',
        reviewed: false,
        autoGenerated: true,
      });
      results.push({ success: true, created: saved });
    }
    // Return first success or error
    const firstSuccess = results.find(r => r.success);
    if (firstSuccess) return firstSuccess;
    return results[0] || { success: false, error: 'No valid scenes created.' };
  }

  // Parse the LLM's batch output and map text to fragments with start/end indices
  parseBatchFrameResponse(response: string): AssignFrame[] {
    const ALLOWED_QUALITY_TYPES = [
      'embodied',
      'attentional',
      'emotional',
      'purposive',
      'spatial',
      'temporal',
      'relational'
    ];
    try {
      const jsonMatch = response.match(/\[.*\]/s);
      if (!jsonMatch) throw new Error('No JSON array found');
      const arr = JSON.parse(jsonMatch[0]);
      if (!Array.isArray(arr)) return [];
      const validFrames: AssignFrame[] = [];
      for (const frameObj of arr) {
        // Validate required fields
        if (!frameObj || typeof frameObj !== 'object') continue;
        const { emoji, summary, shot, qualities, sources } = frameObj;
        if (
          typeof emoji !== 'string' ||
          typeof summary !== 'string' ||
          typeof shot !== 'string' ||
          !Array.isArray(qualities) ||
          !Array.isArray(sources)
        ) {
          addDebugLog(`[parseBatchFrameResponse] Skipping invalid frame: missing required fields.`);
          continue;
        }
        // Filter qualities to allowed types only
        const filteredQualities = qualities.filter((q: any) => ALLOWED_QUALITY_TYPES.includes(q.type));
        if (filteredQualities.length < qualities.length) {
          addDebugLog(`[parseBatchFrameResponse] Dropped qualities with invalid types: ${JSON.stringify(qualities.filter((q: any) => !ALLOWED_QUALITY_TYPES.includes(q.type)))}`);
        }
        // Validate fragments
        // Accept fragments with only sourceId and text (no start/end)
        const validSources: any[] = [];
        for (const frag of sources) {
          if (!frag || typeof frag !== 'object') continue;
          if (typeof frag.sourceId !== 'string' || typeof frag.text !== 'string') continue;
          validSources.push({ sourceId: frag.sourceId, text: frag.text });
        }
        if (validSources.length === 0) {
          addDebugLog(`[parseBatchFrameResponse] Skipping frame with no valid fragments: ${JSON.stringify(frameObj)}`);
          continue;
        }
        validFrames.push({
          emoji,
          summary,
          shot: shot as ShotType,
          qualities: filteredQualities,
          sources: validSources
        });
      }
      return validFrames;
    } catch (e) {
      addDebugLog(`[parseBatchFrameResponse] Failed to parse response: ${e}`);
      return [];
    }
  }

  // Validate: all text present, all content covered, qualities supported by fragment text
  async validateBatchFrames(frames: AssignFrame[], batch: SourceRecord[]): Promise<{ errors: string[]; warnings: string[] }> {
    const errors: string[] = [];
    const warnings: string[] = [];
    // Track coverage for each source
    const coverage: Record<string, boolean[]> = {};
    for (const src of batch) {
      coverage[src.id] = Array(src.content.length).fill(false);
    }
    for (const frame of frames) {
      for (const frag of frame.sources) {
        const src = batch.find(s => s.id === frag.sourceId);
        if (!src) {
          errors.push(`Source not found for fragment: ${frag.sourceId}`);
          continue;
        }
        // Support fragments with only text (no start/end)
        if (typeof frag.text === 'string' && frag.text.length > 0) {
          let searchStart = 0;
          let found = false;
          // Allow minor whitespace/ellipsis differences at start/end
          const clean = (s: string) => s.trim().replace(/^\.*|\.*$/g, '').replace(/\s+/g, ' ');
          const fragTextClean = clean(frag.text);
          while (searchStart < src.content.length) {
            const idx = src.content.indexOf(frag.text, searchStart);
            // Try exact match first
            if (idx !== -1) {
              found = true;
              for (let i = idx; i < idx + frag.text.length; i++) {
                coverage[src.id][i] = true;
              }
              searchStart = idx + frag.text.length;
              break;
            }
            // Try fuzzy match: ignore leading/trailing ellipses and whitespace
            const regex = new RegExp(fragTextClean.replace(/[.*+?^${}()|[\]\\]/g, '\\$&'), 'i');
            const match = regex.exec(src.content);
            if (match) {
              found = true;
              const idx2 = match.index;
              for (let i = idx2; i < idx2 + match[0].length; i++) {
                coverage[src.id][i] = true;
              }
              searchStart = idx2 + match[0].length;
              break;
            }
            break;
          }
          if (!found) {
            errors.push(`Fragment text not found in source: "${frag.text}"`);
          }
        } else if (typeof frag.start === 'number' && typeof frag.end === 'number') {
          if (frag.start < 0 || frag.end > src.content.length || frag.start >= frag.end) {
            errors.push(`Invalid fragment range for source ${frag.sourceId}: [${frag.start}, ${frag.end})`);
            continue;
          }
          for (let i = frag.start; i < frag.end; i++) {
            coverage[frag.sourceId][i] = true;
          }
          const fragText = src.content.slice(frag.start, frag.end);
          if (frame.summary && !fragText) {
            errors.push(`Empty fragment text for summary: ${frame.summary}`);
          }
        }
      }
    }
    // Check for gaps in coverage
    for (const src of batch) {
      let inGap = false, gapStart = 0;
      for (let i = 0; i < src.content.length; i++) {
        if (!coverage[src.id][i] && !inGap) {
          inGap = true;
          gapStart = i;
        } else if (coverage[src.id][i] && inGap) {
          inGap = false;
          const gapText = src.content.slice(gapStart, i);
          if (gapText.trim().length > 0) {
            // Only treat as error if gap is large (>40 chars or >10% of source)
            if (gapText.length > 40 || gapText.length > 0.1 * src.content.length) {
              errors.push(`Large unframed text in source ${src.id} at [${gapStart}, ${i}): "${gapText.trim()}"`);
            } else {
              warnings.push(`Unframed text in source ${src.id} at [${gapStart}, ${i}): "${gapText.trim()}"`);
            }
          }
        }
      }
      if (inGap) {
        const gapText = src.content.slice(gapStart);
        if (gapText.trim().length > 0) {
          if (gapText.length > 40 || gapText.length > 0.1 * src.content.length) {
            errors.push(`Large unframed text in source ${src.id} at [${gapStart}, ${src.content.length}): "${gapText.trim()}"`);
          } else {
            warnings.push(`Unframed text in source ${src.id} at [${gapStart}, ${src.content.length}): "${gapText.trim()}"`);
          }
        }
      }
    }
    return { errors, warnings };
  }

  // TODO: Implement createBatchFramePrompt in prompts.js if not yet present
} 